# -*- coding: utf-8 -*-
"""biencoder

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IYEFF83xBSMqj5mdLjHxjboINZ2FilVk
"""

from transformers import AutoModel, AutoTokenizer, TAPAS_TOKENIZER_CLASS, TAPAS_MODEL_CLASS
import torch

class BiEncoder:
    def __init__(self, query_model_name, context_model_name):
        self.query_model = AutoModel.from_pretrained(query_model_name)
        self.query_tokenizer = AutoTokenizer.from_pretrained(query_model_name)


        if 'tapas' in context_model_name.lower():
            self.context_tokenizer = TAPAS_TOKENIZER_CLASS.from_pretrained(context_model_name)
            self.context_model = TAPAS_MODEL_CLASS.from_pretrained(context_model_name)
        else:
            self.context_tokenizer = AutoTokenizer.from_pretrained(context_model_name)
            self.context_model = AutoModel.from_pretrained(context_model_name)

    def encode(self, texts, tokenizer, model, is_query=True, max_length=512):
        inputs = tokenizer(texts, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")
        token_type_ids = inputs.get("token_type_ids", None)
        attention_mask = inputs.get("attention_mask", None)


        with torch.no_grad():
            if is_query:
                outputs = model(**inputs)
            else:
                outputs = model(input_ids=inputs["input_ids"], attention_mask=attention_mask, token_type_ids=token_type_ids)


        if hasattr(outputs, 'pooler_output'):
            return outputs.pooler_output
        else:
            return outputs.last_hidden_state[:, 0, :]

    def encode_queries(self, queries, max_length=512):
        return self.encode(queries, self.query_tokenizer, self.query_model, is_query=True, max_length=max_length)

    def encode_contexts(self, contexts, max_length=512):
        return self.encode(contexts, self.context_tokenizer, self.context_model, is_query=False, max_length=max_length)

def cosine_similarity(a, b):
    normalized_a = a / a.norm(dim=1)[:, None]
    normalized_b = b / b.norm(dim=1)[:, None]
    return torch.mm(normalized_a, normalized_b.transpose(0, 1))